{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../data')\n",
    "import semantic_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN MERGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved for year 2014\n"
     ]
    }
   ],
   "source": [
    "#perform roberta and lexicon analysis\n",
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    file_path = os.path.join(base_path, f\"cnn_{year}_output.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "\n",
    "\n",
    "        semantic_analysis.lexicon_nltk(file_path)\n",
    "        semantic_analysis.roberta_semantic_algorithm_twitter(file_path)\n",
    "\n",
    "        print(f\"File saved for year {year}\")\n",
    "    else:\n",
    "        print(f\"No file found for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved for year 2014\n",
      "File saved for year 2015\n",
      "File saved for year 2016\n",
      "File saved for year 2017\n",
      "File saved for year 2018\n",
      "File saved for year 2019\n"
     ]
    }
   ],
   "source": [
    "#drop column \n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    file_path_2 = os.path.join(base_path, f\"cnn_{year}_semantics_rob.csv\")\n",
    "\n",
    "    if os.path.exists(file_path_2):\n",
    "\n",
    "\n",
    "        df = pd.read_csv(file_path_2)\n",
    "\n",
    "        df.drop(columns=['Sentiment_Lexicon'], inplace=True)\n",
    "\n",
    "        df.to_csv\n",
    "\n",
    "        print(f\"File saved for year {year}\")\n",
    "    else:\n",
    "        print(f\"No file found for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved for year 2014\n",
      "File saved for year 2015\n",
      "File saved for year 2016\n",
      "File saved for year 2017\n",
      "File saved for year 2018\n",
      "File saved for year 2019\n"
     ]
    }
   ],
   "source": [
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    df = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\cnn_final_output_{year}.csv\")\n",
    "    \n",
    "    #perform the weighted transformation for roberta\n",
    "    df['Sentiment value roberta'] = semantic_analysis.weighted_trans_rob(df)\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(os.path.join(base_path, f\"cnn_{year}_semantics_rob.csv\"), index=False)\n",
    "    \n",
    "    print(f\"File saved for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved for year 2014\n",
      "File saved for year 2015\n",
      "File saved for year 2016\n",
      "File saved for year 2017\n",
      "File saved for year 2018\n",
      "File saved for year 2019\n"
     ]
    }
   ],
   "source": [
    "#merge both files\n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    file_path = os.path.join(base_path, f\"cnn_{year}_semantics_lex.csv\")\n",
    "    file_path_2 = os.path.join(base_path, f\"cnn_{year}_semantics_rob.csv\")\n",
    "    # Check if the file exists for the given year\n",
    "    if os.path.exists(file_path):\n",
    "        semantic_lex = pd.read_csv(file_path)\n",
    "        semantic_rob = pd.read_csv(file_path_2)\n",
    "\n",
    "        merged_df = pd.merge(semantic_lex, semantic_rob, on=['ID', 'Headline', 'Date', 'Organization', 'Link'] )\n",
    "\n",
    "        merged_df.to_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\cnn_final_output_{year}.csv\", index=False)\n",
    "\n",
    "        print(f\"File saved for year {year}\")\n",
    "    else:\n",
    "        print(f\"No file found for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved\n",
      "File saved\n",
      "File saved\n",
      "File saved\n",
      "File saved\n",
      "File saved\n"
     ]
    }
   ],
   "source": [
    "for year in range(2014, 2020):\n",
    "\n",
    "    file_path_2 = os.path.join(base_path, f\"cnn_final_output_{year}.csv\")\n",
    "\n",
    "    if os.path.exists(file_path_2):\n",
    "\n",
    "\n",
    "        df = pd.read_csv(file_path_2)\n",
    "\n",
    "        df.drop(columns=['Sentiment_Lexicon_x'], inplace=True)\n",
    "\n",
    "        df.to_csv(os.path.join(base_path, f\"cnn_final_output_{year}.csv\"), index=False)\n",
    "\n",
    "        print(f\"File saved\")\n",
    "    else:\n",
    "        print(f\"No file found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REUTERS MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform roberta and lexicon analysis\n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2015):\n",
    "\n",
    "    file_path = os.path.join(base_path, f\"cnn_{year}_output.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "\n",
    "\n",
    "        semantic_analysis.lexicon_nltk(file_path)\n",
    "        semantic_analysis.roberta_semantic_algorithm_twitter(file_path)\n",
    "\n",
    "        print(f\"File saved for year {year}\")\n",
    "    else:\n",
    "        print(f\"No file found for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add lexicon and some cleaning I needed to do \n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    df = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\reuters_final_output_{year}.csv\")\n",
    "    columns_order = list(df.columns)\n",
    "    if all(col in columns_order for col in ['Semantic roberta twitter', 'Sentiment scores lexicon']):\n",
    "        # Remove 'Sentiment value roberta.2' if it already exists to re-insert it in the desired position\n",
    "        columns_order = [col for col in columns_order if col != 'Sentiment value roberta.2']\n",
    "        \n",
    "        # Find the index of 'Sentiment scores lexicon' and place 'Sentiment value roberta.2' after it\n",
    "        idx_sentiment_scores = columns_order.index('Sentiment scores lexicon')\n",
    "        columns_order.insert(idx_sentiment_scores, 'Sentiment value roberta.2')\n",
    "        \n",
    "        # Reorder the DataFrame based on the new column order\n",
    "        df = df[columns_order]\n",
    "        \n",
    "        # Save the modified DataFrame back to the CSV file\n",
    "        df.to_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\reuters_final_output_{year}.csv\", index=False)\n",
    "    print(f\"File saved for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also roberta didn't have the weighted sum\n",
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    df = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\reuters_final_output_{year}.csv\")\n",
    "\n",
    "    df.columns.values[7] = 'Sentiment label roberta'\n",
    "    \n",
    "    df['Sentiment value roberta'] = semantic_analysis.check_weighted_sum_consistency(df)\n",
    "\n",
    "    columns = list(df.columns)\n",
    "    insert_index = columns.index('Sentiment scores lexicon')  # Find the index to insert the new column\n",
    "    \n",
    "    # Insert 'Sentiment value roberta' at the appropriate position\n",
    "    columns.insert(insert_index, 'Sentiment value roberta')\n",
    "    df = df[columns]  # Reorder columns\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\computational_ds\\src\\semantic_analysis\\reuters_final_output_{year}.csv\", index=False)\n",
    "    \n",
    "    print(f\"File saved for year {year}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge reuters\n",
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "all_data = []\n",
    "\n",
    "for year in range(2014, 2020):\n",
    "    file_path = os.path.join(base_path, f\"reuters_final_output_{year}.csv\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        data = pd.read_csv(file_path)\n",
    "        all_data.append(data)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "merged_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_data.to_csv(os.path.join(base_path, \"reuters_final_output.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGE FOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved for year 2014\n",
      "File saved for year 2015\n",
      "File saved for year 2016\n"
     ]
    }
   ],
   "source": [
    "#PERFOM LEXICON\n",
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2017):\n",
    "\n",
    "    file_path = os.path.join(base_path, f\"fox_{year}_output.csv\")\n",
    "    \n",
    "    # Check if the file exists for the given year\n",
    "    if os.path.exists(file_path):\n",
    "        # Read the CSV file\n",
    "        semantic_analysis.lexicon_nltk(file_path)\n",
    "        #semantic_analysis.roberta_semantic_algorithm_twitter(file_path)\n",
    "        \n",
    "        print(f\"File saved for year {year}\")\n",
    "    else:\n",
    "        print(f\"No file found for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved for year 2014\n",
      "File saved for year 2015\n",
      "File saved for year 2016\n",
      "File saved for year 2017\n",
      "File saved for year 2018\n",
      "File saved for year 2019\n"
     ]
    }
   ],
   "source": [
    "for year in range(2014, 2020):\n",
    "\n",
    "    file_path = os.path.join(base_path, f\"fox_{year}_semantics_lex.csv\")\n",
    "    file_path_2 = os.path.join(base_path, f\"fox_{year}_semantics_rob.csv\")\n",
    "    # Check if the file exists for the given year\n",
    "    if os.path.exists(file_path):\n",
    "        semantic_lex = pd.read_csv(file_path)\n",
    "        semantic_rob = pd.read_csv(file_path_2)\n",
    "\n",
    "        merged_df = pd.merge(semantic_lex, semantic_rob, on=['ID', 'Headline', 'Date', 'Organization', 'Link'] )\n",
    "\n",
    "        merged_df.to_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\fox_final_output_{year}.csv\", index=False)\n",
    "\n",
    "        print(f\"File saved for year {year}\")\n",
    "    else:\n",
    "        print(f\"No file found for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved for year 2014\n",
      "File saved for year 2015\n",
      "File saved for year 2016\n",
      "File saved for year 2017\n",
      "File saved for year 2018\n",
      "File saved for year 2019\n"
     ]
    }
   ],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    df = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\fox_final_output_{year}.csv\")\n",
    "    \n",
    "    df['Sentiment values roberta'] = semantic_analysis.check_weighted_sum_consistency(df)\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\fox_final_output_{year}.csv\", index=False)\n",
    "    \n",
    "    print(f\"File saved for year {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge fox\n",
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "all_data = []\n",
    "\n",
    "for year in range(2017, 2020):\n",
    "    file_path = os.path.join(base_path, f\"fox_final_output_{year}.csv\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        data = pd.read_csv(file_path)\n",
    "        all_data.append(data)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "merged_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_data.to_csv(os.path.join(base_path, \"fox_final_output.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved\n",
      "File saved\n",
      "File saved\n"
     ]
    }
   ],
   "source": [
    "#drop columns sentiment_lexicon_x, sentiment_lexicon_y\n",
    "\n",
    "for year in range(2017, 2020):\n",
    "    file_path = os.path.join(base_path, f\"fox_final_output_{year}.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        df.drop(columns=['Sentiment_Lexicon_x'], inplace=True)\n",
    "\n",
    "        df.to_csv(os.path.join(base_path, f\"fox_final_output_{year}.csv\"), index=False)\n",
    "\n",
    "        print(f\"File saved\")\n",
    "    else:\n",
    "        print(f\"No file found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGE TO CLUSTER FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "\n",
    "existing_data = pd.read_csv(os.path.join(base_path, \"updated_dataframe_with_clusters_word2vec.csv\"))\n",
    "empty_columns = ['Semantic values roberta twitter', 'Semantic roberta twitter', 'Sentiment value roberta',\n",
    "                'Sentiment scores lexicon', 'Sentiment value lexicon', 'Sentiment lexicon']\n",
    "\n",
    "\n",
    "# Add empty columns with NaN values\n",
    "for col in empty_columns:\n",
    "    existing_data[col] = pd.Series(dtype=object)\n",
    "# Save the updated DataFrame to a new file or overwrite the existing one\n",
    "\n",
    "existing_data.to_csv(os.path.join(base_path, \"updated_dataframe_with_clusters_word2vec_updated.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "# Load the CSV files\n",
    "cnn_df = pd.read_csv(os.path.join(base_path,'cnn_final_output.csv'))\n",
    "reuters_df = pd.read_csv(os.path.join(base_path,'reuters_final_output.csv'))\n",
    "fox_df = pd.read_csv(os.path.join(base_path,'fox_final_output.csv'))\n",
    "\n",
    "# Concatenate the DataFrames vertically\n",
    "merged_df = pd.concat([cnn_df, reuters_df, fox_df], ignore_index=True)\n",
    "\n",
    "# Write the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv(os.path.join(base_path, \"merged_final_outputs.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved\n"
     ]
    }
   ],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "file_path_2 = os.path.join(base_path, f\"merged_final_outputs.csv\")\n",
    "\n",
    "if os.path.exists(file_path_2):\n",
    "\n",
    "\n",
    "    df = pd.read_csv(file_path_2)\n",
    "\n",
    "    df.drop(columns=['Sentiment_Lexicon_x'], inplace=True)\n",
    "\n",
    "    df.to_csv(os.path.join(base_path, \"merged_final_outputs.csv\"), index=False)\n",
    "\n",
    "    print(f\"File saved\")\n",
    "else:\n",
    "    print(f\"No file found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\inest\\\\OneDrive - Danmarks Tekniske Universitet\\\\Semester I\\\\Computational Tools for Data Science\\\\data\\\\updated_dataframe_with_clusters_word2vec_updated.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\storr\\OneDrive - Danmarks Tekniske Universitet\\Year 1\\Semester 1\\Computational Tools for Data Science\\Project\\computational_ds\\src\\semantic_analysis\\merging.ipynb Cell 23\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/storr/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Year%201/Semester%201/Computational%20Tools%20for%20Data%20Science/Project/computational_ds/src/semantic_analysis/merging.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m output_file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(base_path, \u001b[39m\"\u001b[39m\u001b[39mupdated_dataframe_with_clusters_word2vec_updated.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/storr/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Year%201/Semester%201/Computational%20Tools%20for%20Data%20Science/Project/computational_ds/src/semantic_analysis/merging.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Load existing data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/storr/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Year%201/Semester%201/Computational%20Tools%20for%20Data%20Science/Project/computational_ds/src/semantic_analysis/merging.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m existing_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(output_file_path)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/storr/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Year%201/Semester%201/Computational%20Tools%20for%20Data%20Science/Project/computational_ds/src/semantic_analysis/merging.ipynb#X63sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m file_path_2 \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(base_path, \u001b[39m\"\u001b[39m\u001b[39mmerged_final_outputs.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/storr/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Year%201/Semester%201/Computational%20Tools%20for%20Data%20Science/Project/computational_ds/src/semantic_analysis/merging.ipynb#X63sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(file_path_2):\n",
      "File \u001b[1;32mc:\\Users\\storr\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\storr\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\storr\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\storr\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\storr\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_engine(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\storr\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmemory_map\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mencoding_errors\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstorage_options\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\storr\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39mioargs\u001b[39m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\inest\\\\OneDrive - Danmarks Tekniske Universitet\\\\Semester I\\\\Computational Tools for Data Science\\\\data\\\\updated_dataframe_with_clusters_word2vec_updated.csv'"
     ]
    }
   ],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "output_file_path = os.path.join(base_path, \"updated_dataframe_with_clusters_word2vec_updated.csv\")\n",
    "# Load existing data\n",
    "existing_data = pd.read_csv(output_file_path)\n",
    "\n",
    "file_path_2 = os.path.join(base_path, \"merged_final_outputs.csv\")\n",
    "\n",
    "if os.path.exists(file_path_2):\n",
    "    new_data = pd.read_csv(file_path_2, usecols=lambda col: col != 'ID')\n",
    "\n",
    "    # Columns to consider in the merge\n",
    "    columns_to_merge = ['Headline', 'Organization', 'Link', 'Date', \n",
    "                        'Semantic values roberta twitter', 'Semantic roberta twitter', \n",
    "                        'Sentiment value roberta', 'Sentiment scores lexicon', \n",
    "                        'Sentiment value lexicon', 'Sentiment lexicon']\n",
    "\n",
    "    # Left merge data based on matching columns\n",
    "    merged_data = pd.merge(existing_data, new_data, on=['Headline', 'Organization', 'Link', 'Date'], how='left')\n",
    "\n",
    "    # Overwrite the existing data with merged data for this year\n",
    "    merged_data.to_csv(output_file_path, index=False)\n",
    "    print(f\"File updated\")\n",
    "else:\n",
    "    print(f\"No file found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>Text</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Link</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Semantic values roberta twitter_x</th>\n",
       "      <th>Semantic roberta twitter_x</th>\n",
       "      <th>Sentiment value roberta_x</th>\n",
       "      <th>...</th>\n",
       "      <th>Sentiment lexicon_x</th>\n",
       "      <th>Sentiment_Lexicon_x</th>\n",
       "      <th>Sentiment scores lexicon_y</th>\n",
       "      <th>Sentiment value lexicon_y</th>\n",
       "      <th>Sentiment lexicon_y</th>\n",
       "      <th>Sentiment_Lexicon_y</th>\n",
       "      <th>Semantic values roberta twitter_y</th>\n",
       "      <th>Semantic roberta twitter_y</th>\n",
       "      <th>Sentiment value roberta_y</th>\n",
       "      <th>Sentiment values roberta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>China says it’s building new homegrown aircraf...</td>\n",
       "      <td>Updated 1:09 PM EST, Fri January 1, 2016</td>\n",
       "      <td>Story highlights The U.S. military has long be...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2015/12/31/asia/china-...</td>\n",
       "      <td>619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9549</td>\n",
       "      <td>[0.029 0.901 0.069]</td>\n",
       "      <td>0.038</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.9549</td>\n",
       "      <td>[0.11789864 0.75297797 0.1291234 ]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Hillary Clinton emails: Kissinger, Photoshop a...</td>\n",
       "      <td>Updated 9:34 PM EST, Thu December 31, 2015</td>\n",
       "      <td>Story highlights The State Department released...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2015/12/31/politics/cl...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9529</td>\n",
       "      <td>[0.057 0.879 0.064]</td>\n",
       "      <td>0.007</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.9529</td>\n",
       "      <td>[0.37420064 0.5465969  0.07920244]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>How the stars rang in 2016</td>\n",
       "      <td>Published 9:04 AM EST, Fri January 1, 2016</td>\n",
       "      <td>Story highlights Some stars hit exotic locatio...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2016/01/01/entertainme...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9953</td>\n",
       "      <td>[0.009 0.788 0.203]</td>\n",
       "      <td>0.194</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.9953</td>\n",
       "      <td>[0.02461733 0.5008394  0.47454324]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Smoke still wafting from Dubai’s luxury Addres...</td>\n",
       "      <td>Updated 4:55 PM EST, Fri January 1, 2016</td>\n",
       "      <td>Story highlights NEW: Fire has been contained ...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2016/01/01/middleeast/...</td>\n",
       "      <td>482</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.9890</td>\n",
       "      <td>[0.102 0.849 0.049]</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-0.9890</td>\n",
       "      <td>[0.35776561 0.58108795 0.06114649]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Super PACs backing Cruz to launch TV ad blitz</td>\n",
       "      <td>Published 8:36 PM EST, Thu December 31, 2015</td>\n",
       "      <td>Story highlights Ads supporting Cruz set to ai...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2015/12/31/politics/cr...</td>\n",
       "      <td>1428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9877</td>\n",
       "      <td>[0.009 0.861 0.13 ]</td>\n",
       "      <td>0.121</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.9877</td>\n",
       "      <td>[0.06863631 0.7193004  0.21206321]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>China’s one-child policy goes but heartache re...</td>\n",
       "      <td>Published 8:37 PM EST, Thu December 31, 2015</td>\n",
       "      <td>Story highlights China's two-child policy goes...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2015/12/31/asia/china-...</td>\n",
       "      <td>229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>[0.064 0.856 0.08 ]</td>\n",
       "      <td>0.016</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>[0.6098913  0.3609453  0.02916347]</td>\n",
       "      <td>negative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Bangladesh court hands down death sentences fo...</td>\n",
       "      <td>Updated 5:47 AM EST, Fri January 1, 2016</td>\n",
       "      <td>Story highlights Seven sentenced -- two to dea...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2015/12/31/asia/bangla...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.9986</td>\n",
       "      <td>[0.25  0.719 0.031]</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-0.9986</td>\n",
       "      <td>[0.8468856  0.1481697  0.00494466]</td>\n",
       "      <td>negative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Alabama, Clemson to meet in college football t...</td>\n",
       "      <td>Updated 3:22 AM EST, Fri January 1, 2016</td>\n",
       "      <td>CNN — So much for drama. Alabama and Clemson r...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2015/12/31/us/ncaa-foo...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6533</td>\n",
       "      <td>[0.062 0.851 0.087]</td>\n",
       "      <td>0.025</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6533</td>\n",
       "      <td>[0.03096833 0.56298685 0.4060448 ]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Kanye West drops new song ‘Facts’</td>\n",
       "      <td>Updated 10:58 AM EST, Fri January 1, 2016</td>\n",
       "      <td>Story highlights There may be a new Kanye albu...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2016/01/01/entertainme...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9781</td>\n",
       "      <td>[0.039 0.842 0.119]</td>\n",
       "      <td>0.080</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.9781</td>\n",
       "      <td>[0.05871982 0.79267794 0.14860223]</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Camille Cosby ordered to give deposition in ca...</td>\n",
       "      <td>Updated 11:17 AM EST, Fri January 1, 2016</td>\n",
       "      <td>Story highlights Camille Cosby is subpoenaed i...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://edition.cnn.com/2016/01/01/us/camille-...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.9810</td>\n",
       "      <td>[0.123 0.819 0.058]</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-0.9810</td>\n",
       "      <td>[0.49592295 0.4826576  0.02141947]</td>\n",
       "      <td>negative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                           Headline  \\\n",
       "0   1  China says it’s building new homegrown aircraf...   \n",
       "1   2  Hillary Clinton emails: Kissinger, Photoshop a...   \n",
       "2   3                         How the stars rang in 2016   \n",
       "3   4  Smoke still wafting from Dubai’s luxury Addres...   \n",
       "4   5      Super PACs backing Cruz to launch TV ad blitz   \n",
       "5   6  China’s one-child policy goes but heartache re...   \n",
       "6   7  Bangladesh court hands down death sentences fo...   \n",
       "7   8  Alabama, Clemson to meet in college football t...   \n",
       "8   9                  Kanye West drops new song ‘Facts’   \n",
       "9  10  Camille Cosby ordered to give deposition in ca...   \n",
       "\n",
       "                                           Date  \\\n",
       "0      Updated 1:09 PM EST, Fri January 1, 2016   \n",
       "1    Updated 9:34 PM EST, Thu December 31, 2015   \n",
       "2    Published 9:04 AM EST, Fri January 1, 2016   \n",
       "3      Updated 4:55 PM EST, Fri January 1, 2016   \n",
       "4  Published 8:36 PM EST, Thu December 31, 2015   \n",
       "5  Published 8:37 PM EST, Thu December 31, 2015   \n",
       "6      Updated 5:47 AM EST, Fri January 1, 2016   \n",
       "7      Updated 3:22 AM EST, Fri January 1, 2016   \n",
       "8     Updated 10:58 AM EST, Fri January 1, 2016   \n",
       "9     Updated 11:17 AM EST, Fri January 1, 2016   \n",
       "\n",
       "                                                Text Organization  \\\n",
       "0  Story highlights The U.S. military has long be...          CNN   \n",
       "1  Story highlights The State Department released...          CNN   \n",
       "2  Story highlights Some stars hit exotic locatio...          CNN   \n",
       "3  Story highlights NEW: Fire has been contained ...          CNN   \n",
       "4  Story highlights Ads supporting Cruz set to ai...          CNN   \n",
       "5  Story highlights China's two-child policy goes...          CNN   \n",
       "6  Story highlights Seven sentenced -- two to dea...          CNN   \n",
       "7  CNN — So much for drama. Alabama and Clemson r...          CNN   \n",
       "8  Story highlights There may be a new Kanye albu...          CNN   \n",
       "9  Story highlights Camille Cosby is subpoenaed i...          CNN   \n",
       "\n",
       "                                                Link  Cluster  \\\n",
       "0  https://edition.cnn.com/2015/12/31/asia/china-...      619   \n",
       "1  https://edition.cnn.com/2015/12/31/politics/cl...       -1   \n",
       "2  https://edition.cnn.com/2016/01/01/entertainme...       -1   \n",
       "3  https://edition.cnn.com/2016/01/01/middleeast/...      482   \n",
       "4  https://edition.cnn.com/2015/12/31/politics/cr...     1428   \n",
       "5  https://edition.cnn.com/2015/12/31/asia/china-...      229   \n",
       "6  https://edition.cnn.com/2015/12/31/asia/bangla...       -1   \n",
       "7  https://edition.cnn.com/2015/12/31/us/ncaa-foo...       -1   \n",
       "8  https://edition.cnn.com/2016/01/01/entertainme...       -1   \n",
       "9  https://edition.cnn.com/2016/01/01/us/camille-...       -1   \n",
       "\n",
       "   Semantic values roberta twitter_x  Semantic roberta twitter_x  \\\n",
       "0                                NaN                         NaN   \n",
       "1                                NaN                         NaN   \n",
       "2                                NaN                         NaN   \n",
       "3                                NaN                         NaN   \n",
       "4                                NaN                         NaN   \n",
       "5                                NaN                         NaN   \n",
       "6                                NaN                         NaN   \n",
       "7                                NaN                         NaN   \n",
       "8                                NaN                         NaN   \n",
       "9                                NaN                         NaN   \n",
       "\n",
       "   Sentiment value roberta_x  ...  Sentiment lexicon_x  Sentiment_Lexicon_x  \\\n",
       "0                        NaN  ...                  NaN               0.9549   \n",
       "1                        NaN  ...                  NaN               0.9529   \n",
       "2                        NaN  ...                  NaN               0.9953   \n",
       "3                        NaN  ...                  NaN              -0.9890   \n",
       "4                        NaN  ...                  NaN               0.9877   \n",
       "5                        NaN  ...                  NaN               0.9565   \n",
       "6                        NaN  ...                  NaN              -0.9986   \n",
       "7                        NaN  ...                  NaN               0.6533   \n",
       "8                        NaN  ...                  NaN               0.9781   \n",
       "9                        NaN  ...                  NaN              -0.9810   \n",
       "\n",
       "   Sentiment scores lexicon_y  Sentiment value lexicon_y Sentiment lexicon_y  \\\n",
       "0         [0.029 0.901 0.069]                      0.038             neutral   \n",
       "1         [0.057 0.879 0.064]                      0.007             neutral   \n",
       "2         [0.009 0.788 0.203]                      0.194             neutral   \n",
       "3         [0.102 0.849 0.049]                     -0.053             neutral   \n",
       "4         [0.009 0.861 0.13 ]                      0.121             neutral   \n",
       "5         [0.064 0.856 0.08 ]                      0.016             neutral   \n",
       "6         [0.25  0.719 0.031]                     -0.219             neutral   \n",
       "7         [0.062 0.851 0.087]                      0.025             neutral   \n",
       "8         [0.039 0.842 0.119]                      0.080             neutral   \n",
       "9         [0.123 0.819 0.058]                     -0.065             neutral   \n",
       "\n",
       "   Sentiment_Lexicon_y   Semantic values roberta twitter_y  \\\n",
       "0               0.9549  [0.11789864 0.75297797 0.1291234 ]   \n",
       "1               0.9529  [0.37420064 0.5465969  0.07920244]   \n",
       "2               0.9953  [0.02461733 0.5008394  0.47454324]   \n",
       "3              -0.9890  [0.35776561 0.58108795 0.06114649]   \n",
       "4               0.9877  [0.06863631 0.7193004  0.21206321]   \n",
       "5               0.9565  [0.6098913  0.3609453  0.02916347]   \n",
       "6              -0.9986  [0.8468856  0.1481697  0.00494466]   \n",
       "7               0.6533  [0.03096833 0.56298685 0.4060448 ]   \n",
       "8               0.9781  [0.05871982 0.79267794 0.14860223]   \n",
       "9              -0.9810  [0.49592295 0.4826576  0.02141947]   \n",
       "\n",
       "   Semantic roberta twitter_y Sentiment value roberta_y  \\\n",
       "0                     neutral                       NaN   \n",
       "1                     neutral                       NaN   \n",
       "2                     neutral                       NaN   \n",
       "3                     neutral                       NaN   \n",
       "4                     neutral                       NaN   \n",
       "5                    negative                       NaN   \n",
       "6                    negative                       NaN   \n",
       "7                     neutral                       NaN   \n",
       "8                     neutral                       NaN   \n",
       "9                    negative                       NaN   \n",
       "\n",
       "  Sentiment values roberta  \n",
       "0                      NaN  \n",
       "1                      NaN  \n",
       "2                      NaN  \n",
       "3                      NaN  \n",
       "4                      NaN  \n",
       "5                      NaN  \n",
       "6                      NaN  \n",
       "7                      NaN  \n",
       "8                      NaN  \n",
       "9                      NaN  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "df = pd.read_csv(os.path.join(base_path, \"updated_dataframe_with_clusters_and_semantics.csv\"))\n",
    "\n",
    "\n",
    "excluded_organizations = [\"CNN\", \"FOX\", \"Reuters\"]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['Organization'] not in excluded_organizations:\n",
    "        df.at[index, 'Organization'], df.at[index, 'Link'] = row['Link'], row['Organization']\n",
    "\n",
    "df.to_csv(os.path.join(base_path, 'updated_dataframe_with_clusters_and_semantics.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "file_path_2 = os.path.join(base_path, \"updated_dataframe_with_clusters_and_semantics.csv\")\n",
    "\n",
    "if os.path.exists(file_path_2):\n",
    "    df = semantic_analysis.sampling_articles(file_path_2)\n",
    "\n",
    "    df.to_csv(os.path.join(base_path, \"sampling_articles.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\"\n",
    "\n",
    "file_path_2 = os.path.join(base_path, \"updated_dataframe_with_clusters_and_semantics.csv\")\n",
    "\n",
    "if os.path.exists(file_path_2):\n",
    "\n",
    "    df = pd.read_csv(file_path_2)\n",
    "\n",
    "    df.isna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0    ID                                           Headline  \\\n",
      "86333        86333   171        Hawaii girl, 3, dies after dental procedure   \n",
      "93274        93274  7095              Vanishing spray makes World Cup debut   \n",
      "100558      100558     1  BRIEF-Anhui Jianghuai Automobile Group Corp re...   \n",
      "100559      100559     2  BRIEF-Wi-Lan, Funai Electric enter license agr...   \n",
      "100560      100560     3  Nikkei tumbles to 2-1/2-month lows on weak Chi...   \n",
      "\n",
      "                                             Date Organization  \\\n",
      "86333   Updated 12:06 PM EST, Sun January 5, 2014          CNN   \n",
      "93274    Published 5:47 PM EDT, Thu June 12, 2014          CNN   \n",
      "100558                       2016-12-29T06:45:37Z      Reuters   \n",
      "100559                       2016-12-29T15:06:56Z      Reuters   \n",
      "100560                       2016-01-04T02:41:05Z      Reuters   \n",
      "\n",
      "                                                     Link  Cluster  \\\n",
      "86333   https://edition.cnn.com/2014/01/04/justice/haw...       -1   \n",
      "93274   https://edition.cnn.com/2014/06/12/tech/innova...       -1   \n",
      "100558      https://www.reuters.com/article/idUSL4N1EO1ZP      421   \n",
      "100559       https://www.reuters.com/article/idUSASC09P51      420   \n",
      "100560  https://www.reuters.com/article/japan-stocks-m...       -1   \n",
      "\n",
      "        Sentiment value lexicon Sentiment lexicon Semantic roberta twitter  \\\n",
      "86333                    -0.089           neutral                 negative   \n",
      "93274                     0.041           neutral                  neutral   \n",
      "100558                    0.062           neutral                  neutral   \n",
      "100559                    0.162           neutral                  neutral   \n",
      "100560                    0.073           neutral                  neutral   \n",
      "\n",
      "        Sentiment values roberta  \n",
      "86333                        NaN  \n",
      "93274                        NaN  \n",
      "100558                       NaN  \n",
      "100559                       NaN  \n",
      "100560                       NaN  \n"
     ]
    }
   ],
   "source": [
    "print(df[df['Sentiment values roberta'].isna()][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Organization                       0\n",
      "Link                               0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n",
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Organization                       0\n",
      "Link                               0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n",
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Organization                       0\n",
      "Link                               0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n",
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Organization                       0\n",
      "Link                               0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n",
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Organization                       0\n",
      "Link                               0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n",
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Organization                       0\n",
      "Link                               0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for year in range(2014, 2020):\n",
    "\n",
    "    df = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\cnn_final_output_{year}.csv\")\n",
    "    \n",
    "    print(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                  0\n",
      "ID                          0\n",
      "Headline                    0\n",
      "Date                        0\n",
      "Organization                0\n",
      "Link                        0\n",
      "Cluster                     0\n",
      "Sentiment value lexicon     0\n",
      "Sentiment lexicon           0\n",
      "Semantic roberta twitter    0\n",
      "Sentiment values roberta    2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(rf\"C:\\Users\\inest\\OneDrive - Danmarks Tekniske Universitet\\Semester I\\Computational Tools for Data Science\\data\\updated_dataframe_with_clusters_and_semantics.csv\")\n",
    "    \n",
    "print(df[df['Organization']=='Reuters'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGING SEMANTICS TO MASTER FILE WITH CLUSTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Organization                       0\n",
      "Link                               0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n",
      "ID                                    0\n",
      "Headline                              0\n",
      "Date                               2814\n",
      "Link                                  0\n",
      "Organization                          0\n",
      "Sentiment scores lexicon              0\n",
      "Sentiment value lexicon               0\n",
      "Sentiment lexicon                     0\n",
      "Semantic values roberta twitter       0\n",
      "Semantic roberta twitter              0\n",
      "Sentiment values roberta              0\n",
      "dtype: int64\n",
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Organization                       0\n",
      "Link                               0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n",
      "ID                                    0\n",
      "Headline                              0\n",
      "Date                               5474\n",
      "Link                                  0\n",
      "Organization                          0\n",
      "Sentiment scores lexicon              0\n",
      "Sentiment value lexicon               0\n",
      "Sentiment lexicon                     0\n",
      "Semantic values roberta twitter       0\n",
      "Semantic roberta twitter              0\n",
      "Sentiment values roberta              0\n",
      "dtype: int64\n",
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Organization                       0\n",
      "Link                               0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n",
      "ID                                    0\n",
      "Headline                              0\n",
      "Date                               6096\n",
      "Link                                  0\n",
      "Organization                          0\n",
      "Sentiment scores lexicon              0\n",
      "Sentiment value lexicon               0\n",
      "Sentiment lexicon                     0\n",
      "Semantic values roberta twitter       0\n",
      "Semantic roberta twitter              0\n",
      "Sentiment values roberta              0\n",
      "dtype: int64\n",
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Organization                       0\n",
      "Link                               0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n",
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Link                               0\n",
      "Organization                       0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n",
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Organization                       0\n",
      "Link                               0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n",
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Link                               0\n",
      "Organization                       0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n",
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Organization                       0\n",
      "Link                               0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n",
      "ID                                 0\n",
      "Headline                           0\n",
      "Date                               0\n",
      "Link                               0\n",
      "Organization                       0\n",
      "Sentiment scores lexicon           0\n",
      "Sentiment value lexicon            0\n",
      "Sentiment lexicon                  0\n",
      "Semantic values roberta twitter    0\n",
      "Semantic roberta twitter           0\n",
      "Sentiment values roberta           0\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 11 fields in line 7593, saw 12\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\storr\\OneDrive - Danmarks Tekniske Universitet\\Year 1\\Semester 1\\Computational Tools for Data Science\\Project\\computational_ds\\src\\semantic_analysis\\merging.ipynb Cell 32\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/storr/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Year%201/Semester%201/Computational%20Tools%20for%20Data%20Science/Project/computational_ds/src/semantic_analysis/merging.ipynb#Y112sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m desired_order \u001b[39m=\u001b[39m [\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/storr/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Year%201/Semester%201/Computational%20Tools%20for%20Data%20Science/Project/computational_ds/src/semantic_analysis/merging.ipynb#Y112sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mID\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/storr/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Year%201/Semester%201/Computational%20Tools%20for%20Data%20Science/Project/computational_ds/src/semantic_analysis/merging.ipynb#Y112sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mHeadline\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/storr/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Year%201/Semester%201/Computational%20Tools%20for%20Data%20Science/Project/computational_ds/src/semantic_analysis/merging.ipynb#Y112sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSentiment values roberta\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/storr/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Year%201/Semester%201/Computational%20Tools%20for%20Data%20Science/Project/computational_ds/src/semantic_analysis/merging.ipynb#Y112sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m ]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/storr/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Year%201/Semester%201/Computational%20Tools%20for%20Data%20Science/Project/computational_ds/src/semantic_analysis/merging.ipynb#Y112sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(base_path, \u001b[39m\"\u001b[39m\u001b[39mreuters_final_output.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/storr/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Year%201/Semester%201/Computational%20Tools%20for%20Data%20Science/Project/computational_ds/src/semantic_analysis/merging.ipynb#Y112sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m new_data_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(file_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/storr/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Year%201/Semester%201/Computational%20Tools%20for%20Data%20Science/Project/computational_ds/src/semantic_analysis/merging.ipynb#Y112sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m new_data_df \u001b[39m=\u001b[39m new_data_df[desired_order]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/storr/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Year%201/Semester%201/Computational%20Tools%20for%20Data%20Science/Project/computational_ds/src/semantic_analysis/merging.ipynb#Y112sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m data_list\u001b[39m.\u001b[39mappend(new_data_df)\n",
      "File \u001b[1;32mc:\\Users\\storr\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\storr\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\storr\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\storr\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\storr\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1779\u001b[0m         nrows\n\u001b[0;32m   1780\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\storr\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Users\\storr\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\storr\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\storr\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\storr\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 11 fields in line 7593, saw 12\n"
     ]
    }
   ],
   "source": [
    "large_file_path = r'C:\\Users\\storr\\OneDrive - Danmarks Tekniske Universitet\\Year 1\\Semester 1\\Computational Tools for Data Science\\Project\\DATA\\updated_dataframe_with_clusters_word2vec.csv'\n",
    "large_df = pd.read_csv(large_file_path)\n",
    "\n",
    "\n",
    "base_path = r\"C:\\Users\\storr\\OneDrive - Danmarks Tekniske Universitet\\Year 1\\Semester 1\\Computational Tools for Data Science\\Project\\DATA\"\n",
    "data_list = []\n",
    "\n",
    "# Loop through years from 2014 to 2019\n",
    "for year in range(2014, 2020):\n",
    "\n",
    "    file_path = os.path.join(base_path, f\"cnn_final_output_{year}.csv\")\n",
    "    new_data_df = pd.read_csv(file_path)\n",
    "    data_list.append(new_data_df)\n",
    "    print(new_data_df.isna().sum())\n",
    "        \n",
    "    file_path = os.path.join(base_path, f\"fox_final_output_{year}.csv\")\n",
    "    new_data_df = pd.read_csv(file_path)\n",
    "    data_list.append(new_data_df)\n",
    "    print(new_data_df.isna().sum())\n",
    "\n",
    "desired_order = [\n",
    "    'ID',\n",
    "    'Headline',\n",
    "    'Date',\n",
    "    'Organization',\n",
    "    'Link',\n",
    "    'Sentiment scores lexicon',\n",
    "    'Sentiment value lexicon',\n",
    "    'Sentiment lexicon',\n",
    "    'Semantic values roberta twitter',\n",
    "    'Semantic roberta twitter',\n",
    "    'Sentiment values roberta'\n",
    "]\n",
    "\n",
    "file_path = os.path.join(base_path, \"reuters_final_output.csv\")\n",
    "new_data_df = pd.read_csv(file_path)\n",
    "new_data_df = new_data_df[desired_order]\n",
    "data_list.append(new_data_df)\n",
    "print(new_data_df.isna().sum())  \n",
    "\n",
    "data_df = pd.concat(data_list, ignore_index = True)\n",
    "data_df.isna().sum()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                              0\n",
       "Headline                        0\n",
       "Date                        43102\n",
       "Organization                    0\n",
       "Link                            0\n",
       "Cluster                         0\n",
       "Sentiment value lexicon     81352\n",
       "Sentiment lexicon           81352\n",
       "Semantic roberta twitter    81352\n",
       "Sentiment values roberta    81352\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_file_path = r'C:\\Users\\storr\\OneDrive - Danmarks Tekniske Universitet\\Year 1\\Semester 1\\Computational Tools for Data Science\\Project\\DATA\\updated_dataframe_with_clusters_word2vec.csv'\n",
    "large_df = pd.read_csv(large_file_path)\n",
    "large_df = semantic_analysis.merge_data(large_df, data_df, column_1 = 'Sentiment value lexicon', column_2 = 'Sentiment lexicon', column_3= 'Semantic roberta twitter', column_4='Sentiment values roberta')\n",
    "large_df = large_df.drop('Text', axis=1)\n",
    "#large_df = large_df[large_df['Organization']!='Reuters']\n",
    "large_df.to_csv(r\"C:\\Users\\storr\\OneDrive - Danmarks Tekniske Universitet\\Year 1\\Semester 1\\Computational Tools for Data Science\\Project\\DATA\\updated_dataframe_with_clusters_and_semantics.csv\")\n",
    "large_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\storr\\OneDrive - Danmarks Tekniske Universitet\\Year 1\\Semester 1\\Computational Tools for Data Science\\Project\\DATA\"\n",
    "file_path = os.path.join(base_path, \"reuters_selec_cols.csv\")\n",
    "reuters_selec_cols = pd.read_csv(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
